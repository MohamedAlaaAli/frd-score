{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02ff153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhamed/miniconda3/envs/frd/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16495f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbedder:\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vision Transformer embedder without preprocessing.\n",
    "        Expects input tensors to already be normalized and resized correctly.\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ViTModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_embeddings(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get embeddings for a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Tensor of shape (B, C, H, W) in float32,\n",
    "                                   already normalized & resized to 224x224.\n",
    "                                   Range should match model's expected input.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        if images.device != self.device:\n",
    "            images = images.to(self.device)\n",
    "\n",
    "        outputs = self.model(pixel_values=images)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a32e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderPILDataset(Dataset):\n",
    "    def __init__(self, root_dir, extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "        self.root_dir = root_dir\n",
    "        self.extensions = extensions\n",
    "        self.image_paths = [\n",
    "            os.path.join(root, fname)\n",
    "            for root, _, files in os.walk(root_dir)\n",
    "            for fname in files\n",
    "            if fname.lower().endswith(extensions)\n",
    "        ]\n",
    "        if not self.image_paths:\n",
    "            raise ValueError(f\"No images found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "        # Resize and overwrite images on disk\n",
    "        for path in self.image_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = img.resize((224,224), Image.BILINEAR)\n",
    "            img.save(path)  # overwrite on disk\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float()/255.0\n",
    "        return (img, self.image_paths[idx])\n",
    "\n",
    "def get_image_dataloader(root_dir, batch_size=8, num_workers=4, shuffle=False):\n",
    "    dataset = ImageFolderPILDataset(root_dir)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d66d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_image_dataloader(\"frd/frd_v1/datasets/clean/busi\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc155a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vit = ViTEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8de00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0745c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_adv_batch(batch, save_dir=\"frd/frd_v1/datasets/adv/busi\", pth=None):\n",
    "    \"\"\"\n",
    "    Save each tensor in the batch as a .bmp image, keeping numbering across calls.\n",
    "    Assumes images are in [0,1] range (float) or [0,255] (uint8).\n",
    "    \"\"\"\n",
    "\n",
    "    for img, p in zip(batch, pth):\n",
    "        img = img.detach().cpu()\n",
    "\n",
    "        # If image is [C, H, W], convert to [H, W, C]\n",
    "        if img.dim() == 3:\n",
    "            img = img.permute(1, 2, 0)\n",
    "        \n",
    "        # Scale to 0â€“255 and convert to uint8 if needed\n",
    "        if img.dtype != torch.uint8:\n",
    "            img = (img * 255).clamp(0, 255).byte()\n",
    "\n",
    "        img_pil = Image.fromarray(img.numpy())\n",
    "        file_path = os.path.join(save_dir, os.path.basename(p))\n",
    "        img_pil.save(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5629309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_kl_attack(model, x, epsilon, temp=0.5):\n",
    "    \"\"\"\n",
    "    FGSM attack to maximize KL divergence between original and adversarial embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: embedding model (outputs [B, D] embeddings or logits).\n",
    "        x: input tensor [B, ...] with requires_grad=False.\n",
    "        epsilon: L_inf bound (float).\n",
    "        temp: temperature scaling for embeddings before KL (float).\n",
    "    Returns:\n",
    "        x_adv: adversarial example tensor [B, ...].\n",
    "    \"\"\"\n",
    "    # Ensure we work on a copy so the original isn't modified\n",
    "    x, pth = x\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Get clean embeddings\n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "    \n",
    "    # Forward pass for adversarial input\n",
    "    emb_adv = model.get_embeddings(x_adv)  # [B, D]\n",
    "\n",
    "    # Apply temperature scaling + softmax so KL is well-defined\n",
    "    p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "    q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "    # KL divergence (maximize)\n",
    "    loss = F.kl_div(p, q, reduction='batchmean')  # KL(p || q)\n",
    "    loss = -loss  # negate to maximize KL\n",
    "    # Backprop to get gradient wrt inputs\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM step: sign of gradient, scaled by epsilon\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "\n",
    "    # Project back to valid range (e.g., [0,1] for images)\n",
    "    x_adv = torch.clamp(x_adv, 0.0, 1.0).detach()\n",
    "\n",
    "    save_adv_batch(batch=x_adv, pth=pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eba1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    fgsm_kl_attack(model=vit, x=batch, epsilon=4/255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
