{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ff153",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import  differential_evolution\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16495f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEmbedder:\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\", device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the Vision Transformer embedder without preprocessing.\n",
    "        Expects input tensors to already be normalized and resized correctly.\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ViTModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_embeddings(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get embeddings for a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Tensor of shape (B, C, H, W) in float32,\n",
    "                                   already normalized & resized to 224x224.\n",
    "                                   Range should match model's expected input.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        if images.device != self.device:\n",
    "            images = images.to(self.device)\n",
    "\n",
    "        outputs = self.model(pixel_values=images)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a32e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderPILDataset(Dataset):\n",
    "    def __init__(self, root_dir, extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "        self.root_dir = root_dir\n",
    "        self.extensions = extensions\n",
    "        self.image_paths = [\n",
    "            os.path.join(root, fname)\n",
    "            for root, _, files in os.walk(root_dir)\n",
    "            for fname in files\n",
    "            if fname.lower().endswith(extensions)\n",
    "        ]\n",
    "        if not self.image_paths:\n",
    "            raise ValueError(f\"No images found in {root_dir} with extensions {extensions}\")\n",
    "\n",
    "        # Resize and overwrite images on disk\n",
    "        for path in self.image_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img = img.resize((224,224), Image.BILINEAR)\n",
    "            img.save(path)  # overwrite on disk\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float()/255.0\n",
    "        return (img, self.image_paths[idx])\n",
    "\n",
    "def get_image_dataloader(root_dir, batch_size=8, num_workers=4, shuffle=False):\n",
    "    dataset = ImageFolderPILDataset(root_dir)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d66d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_image_dataloader(\"frd/frd_v1/datasets/clean/busi\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc155a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vit = ViTEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8de00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0745c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_adv_batch(batch, save_dir=\"frd/frd_v1/datasets/adv/busi\", pth=None):\n",
    "    \"\"\"\n",
    "    Save each tensor in the batch as a .bmp image, keeping numbering across calls.\n",
    "    Assumes images are in [0,1] range (float) or [0,255] (uint8).\n",
    "    \"\"\"\n",
    "\n",
    "    for img, p in zip(batch, pth):\n",
    "        img = img.detach().cpu()\n",
    "\n",
    "        # If image is [C, H, W], convert to [H, W, C]\n",
    "        if img.dim() == 3:\n",
    "            img = img.permute(1, 2, 0)\n",
    "        \n",
    "        # Scale to 0â€“255 and convert to uint8 if needed\n",
    "        if img.dtype != torch.uint8:\n",
    "            img = (img * 255).clamp(0, 255).byte()\n",
    "\n",
    "        img_pil = Image.fromarray(img.numpy())\n",
    "        file_path = os.path.join(save_dir, os.path.basename(p))\n",
    "        img_pil.save(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5629309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_kl_attack(model, x, epsilon, temp=0.5):\n",
    "    \"\"\"\n",
    "    FGSM attack to maximize KL divergence between original and adversarial embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: embedding model (outputs [B, D] embeddings or logits).\n",
    "        x: input tensor [B, ...] with requires_grad=False.\n",
    "        epsilon: L_inf bound (float).\n",
    "        temp: temperature scaling for embeddings before KL (float).\n",
    "    Returns:\n",
    "        x_adv: adversarial example tensor [B, ...].\n",
    "    \"\"\"\n",
    "    # Ensure we work on a copy so the original isn't modified\n",
    "    x, pth = x\n",
    "    x_adv = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Get clean embeddings\n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "    \n",
    "    # Forward pass for adversarial input\n",
    "    emb_adv = model.get_embeddings(x_adv)  # [B, D]\n",
    "\n",
    "    # Apply temperature scaling + softmax so KL is well-defined\n",
    "    p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "    q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "    # KL divergence (maximize)\n",
    "    loss = F.kl_div(p, q, reduction='batchmean')  # KL(p || q)\n",
    "    loss = -loss  # negate to maximize KL\n",
    "    # Backprop to get gradient wrt inputs\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM step: sign of gradient, scaled by epsilon\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "\n",
    "    # Project back to valid range (e.g., [0,1] for images)\n",
    "    x_adv = torch.clamp(x_adv, 0.0, 1.0).detach()\n",
    "\n",
    "    save_adv_batch(batch=x_adv, pth=pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_kl_attack_dct(model, x, epsilon, temp=0.5, save_dir=None):\n",
    "    \"\"\"\n",
    "    FGSM attack to maximize KL divergence between original and adversarial embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: embedding model (outputs [B, D] embeddings or logits).\n",
    "        x: input tensor [B, ...] with requires_grad=False.\n",
    "        epsilon: L_inf bound (float).\n",
    "        temp: temperature scaling for embeddings before KL (float).\n",
    "    Returns:\n",
    "        x_adv: adversarial example tensor [B, ...].\n",
    "    \"\"\"\n",
    "    # Ensure we work on a copy so the original isn't modified\n",
    "    x, pth = x\n",
    "\n",
    "    # x_adversarial in dct domain\n",
    "    x_adv = dct_2d(x).detach().requires_grad_(True)\n",
    "    x_adv_pixel = idct_2d(x_adv)\n",
    "    \n",
    "    # x_adv in dct --> x_adv in pixel --> kl divergence --> grad \n",
    "\n",
    "    # Get clean embeddings\n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "    \n",
    "    # Forward pass for adversarial input\n",
    "    emb_adv = model.get_embeddings(x_adv_pixel)  # [B, D]\n",
    "\n",
    "    # Apply temperature scaling + softmax so KL is well-defined\n",
    "    p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "    q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "    # KL divergence (maximize)\n",
    "    loss = F.kl_div(p, q, reduction='batchmean')  # KL(p || q)\n",
    "    loss = -loss  # negate to maximize KL\n",
    "    # Backprop to get gradient wrt inputs\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM step: sign of gradient, scaled by epsilon\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "\n",
    "    # Project back to valid range (e.g., [0,1] for images)\n",
    "    x_adv_pixel = torch.clamp(idct_2d(x_adv), 0.0, 1.0).detach()\n",
    "    save_adv_batch(batch=x_adv_pixel, pth=pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a365cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_kl_attack_fourier(model, x, epsilon, temp=0.5, save_dir=None):\n",
    "    \"\"\"\n",
    "    FGSM attack to maximize KL divergence between original and adversarial embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: embedding model (outputs [B, D] embeddings or logits).\n",
    "        x: input tensor [B, ...] with requires_grad=False.\n",
    "        epsilon: L_inf bound (float).\n",
    "        temp: temperature scaling for embeddings before KL (float).\n",
    "    Returns:\n",
    "        x_adv: adversarial example tensor [B, ...].\n",
    "    \"\"\"\n",
    "    # Ensure we work on a copy so the original isn't modified\n",
    "    x, pth = x\n",
    "\n",
    "    # x_adversarial in fourier domain\n",
    "    x_adv = torch.fft.fft2(x) \n",
    "    x_adv = torch.fft.fftshift(x_adv)\n",
    "    x_adv = x_adv.detach().requires_grad_(True)\n",
    "\n",
    "    x_adv_pixel = torch.fft.ifftshift(x_adv)\n",
    "    x_adv_pixel = torch.fft.ifft2(x_adv_pixel).real\n",
    "\n",
    "    # x_adv in dct --> x_adv in pixel --> kl divergence --> grad \n",
    "\n",
    "    # Get clean embeddings\n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "    \n",
    "    # Forward pass for adversarial input\n",
    "    emb_adv = model.get_embeddings(x_adv_pixel)  # [B, D]\n",
    "\n",
    "    # Apply temperature scaling + softmax so KL is well-defined\n",
    "    p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "    q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "    # KL divergence (maximize)\n",
    "    loss = F.kl_div(p, q, reduction='batchmean')  # KL(p || q)\n",
    "    loss = -loss  # negate to maximize KL\n",
    "    # Backprop to get gradient wrt inputs\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM step: sign of gradient, scaled by epsilon\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sgn()\n",
    "\n",
    "    # Project back to valid range (e.g., [0,1] for images)\n",
    "    x_adv_pixel = torch.fft.ifftshift(x_adv)\n",
    "    x_adv_pixel = torch.fft.ifft2(x_adv_pixel).real\n",
    "    x_adv_pixel = torch.clamp(x_adv_pixel, 0.0, 1.0).detach()\n",
    "    save_adv_batch(batch=x_adv_pixel, pth=pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca87c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_kl_attack(model, x, epsilon, alpha=0.01, steps=40, temp=0.5, save_dir=None):\n",
    "    \n",
    "    x, pth = x\n",
    "    x_adv = dct_2d(x)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # We need this because x_adv is moved to a new tensor in each iteration and requires_grad = false\n",
    "        # Could be fixed by updating using x_adv.data  not x_adv directly but let's leave it for learning purposes\n",
    "        x_adv.requires_grad_(True) \n",
    "        emb_adv = model.get_embeddings(x_adv) # [B, D]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "        q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "        # KL divergence (maximize)\n",
    "        loss = F.kl_div(p, q, reduction='batchmean')\n",
    "        loss = -loss\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Update \n",
    "            x_adv = x_adv + alpha * x_adv.grad.sign()\n",
    "            \n",
    "            # Projection \n",
    "            perturbation = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "            x_adv = torch.clamp(x + perturbation, 0.0, 1.0)\n",
    "            \n",
    "            x_adv.grad = None\n",
    "\n",
    "    x_adv = x_adv.detach()\n",
    "    save_adv_batch(batch=x_adv, pth=pth)\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_kl_attack_fourier(model, x, epsilon, alpha=0.01, steps=40, temp=0.5, save_dir=None):\n",
    "    \n",
    "    x, pth = x\n",
    "\n",
    "    x_adv = torch.fft.fft2(x)  # Use fft2 for 2D images\n",
    "    x_adv = torch.fft.fftshift(x_adv)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "\n",
    "    for _ in range(steps):\n",
    "\n",
    "        x_adv.requires_grad_(True) \n",
    "        x_adv_pixel = torch.fft.ifft2(torch.fft.ifftshift(x_adv)).real  # Use ifft2 for 2D\n",
    "\n",
    "        emb_adv = model.get_embeddings(x_adv_pixel) # [B, D]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "        q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "        # KL divergence (maximize)\n",
    "        loss = F.kl_div(p, q, reduction='batchmean')\n",
    "        loss = -loss\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Update in frequency domain\n",
    "            x_adv = x_adv + alpha * torch.sgn(x_adv.grad)\n",
    "            \n",
    "            # Project back to valid constraints\n",
    "            x_adv_pixel_new = torch.fft.ifft2(torch.fft.ifftshift(x_adv)).real  # Use ifft2\n",
    "            perturbation = torch.clamp(x_adv_pixel_new - x, min=-epsilon, max=epsilon)\n",
    "            x_adv_pixel_clipped = torch.clamp(x + perturbation, min=0.0, max=1.0)\n",
    "            \n",
    "            # Convert back to frequency domain for next iteration\n",
    "            x_adv = torch.fft.fftshift(torch.fft.fft2(x_adv_pixel_clipped))\n",
    "            \n",
    "        # Clear gradients for next iteration\n",
    "        if x_adv.grad is not None:\n",
    "            x_adv.grad = None\n",
    "\n",
    "    # Convert final result back to pixel domain\n",
    "    x_adv_final = torch.fft.ifft2(torch.fft.ifftshift(x_adv)).real\n",
    "    x_adv_final = x_adv_final.detach()\n",
    "    \n",
    "    save_adv_batch(batch=x_adv_final, pth=pth)  # Save pixel domain result\n",
    "    return x_adv_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8057e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_kl_attack_dct(model, x, epsilon, alpha=0.01, steps=40, temp=0.5, save_dir=None):\n",
    "    \n",
    "    x, pth = x\n",
    "\n",
    "    x_adv = dct_2d(x)  # Use dct_2d for 2D images\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)  # [B, D]\n",
    "\n",
    "    for _ in range(steps):\n",
    "\n",
    "        x_adv.requires_grad_(True) \n",
    "        x_adv_pixel = idct_2d(x_adv)  # Use idct_2d for 2D\n",
    "\n",
    "        emb_adv = model.get_embeddings(x_adv_pixel) # [B, D]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "        q = F.softmax(emb_adv / temp, dim=-1)\n",
    "\n",
    "        # KL divergence (maximize)\n",
    "        loss = F.kl_div(p, q, reduction='batchmean')\n",
    "        loss = -loss\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Update in dct domain\n",
    "            x_adv = x_adv + alpha * torch.sgn(x_adv.grad)\n",
    "                \n",
    "            # Project back to valid constraints\n",
    "            x_adv_pixel_new = idct_2d(x_adv)\n",
    "            perturbation = torch.clamp(x_adv_pixel_new - x, min=-epsilon, max=epsilon)\n",
    "            x_adv_pixel_clipped = torch.clamp(x + perturbation, min=0.0, max=1.0)\n",
    "            \n",
    "            # Convert back to dct domain for next iteration\n",
    "            x_adv = dct_2d(x_adv_pixel_clipped)\n",
    "\n",
    "        # Clear gradients for next iteration\n",
    "        if x_adv.grad is not None:\n",
    "            x_adv.grad = None\n",
    "\n",
    "    # Convert final result back to pixel domain\n",
    "    x_adv_final = idct_2d(x_adv)\n",
    "    x_adv_final = x_adv_final.detach()\n",
    "    \n",
    "    save_adv_batch(batch=x_adv_final, pth=pth)  # Save pixel domain result\n",
    "    return x_adv_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-level function\n",
    "def objective_function(candidate, x, emb_clean, model, temp):\n",
    "    x_adv = x.clone()\n",
    "    for i in range(x.shape[0]):\n",
    "        candidate_i = candidate[i*5:(i+1)*5]\n",
    "        xi, yi = int(candidate_i[0]), int(candidate_i[1])\n",
    "        color = torch.tensor(candidate_i[2:], device=x.device, dtype=x.dtype)\n",
    "        x_adv[i, :, xi, yi] = color\n",
    "\n",
    "    emb_adv = model.get_embeddings(x_adv)\n",
    "    p = F.log_softmax(emb_clean / temp, dim=-1)\n",
    "    q = F.softmax(emb_adv / temp, dim=-1)\n",
    "    return -F.kl_div(p, q, reduction='batchmean').item()\n",
    "\n",
    "\n",
    "def one_pixel_kl_attack(model, x, temp=0.5):\n",
    "    \"\"\"\n",
    "    One-pixel attack to maximize KL divergence between original and adversarial embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: embedding model (outputs [B, D] embeddings or logits).\n",
    "        x: input tensor [B, ...] with requires_grad=False.\n",
    "        d: number of pixels to perturb (int).\n",
    "        temp: temperature scaling for embeddings before KL (float).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x, pth = x\n",
    "    with torch.no_grad():\n",
    "        emb_clean = model.get_embeddings(x)\n",
    "\n",
    "    # candidate -> array [x, y, color.R, color.G, color.B] * batch_size\n",
    "    bounds = [(0, 223), (0, 223), (0, 1), (0, 1), (0, 1)] * x.shape[0]\n",
    "\n",
    "    result = differential_evolution(\n",
    "        objective_function, \n",
    "        bounds, \n",
    "        args=(x, emb_clean, model, temp),\n",
    "        maxiter=100, \n",
    "        popsize=50, \n",
    "        tol=1e-5, \n",
    "        workers=-1\n",
    "    )\n",
    "\n",
    "    x_adv = x.clone()\n",
    "    for i in range(x.shape[0]):\n",
    "        candidate_i = result.x[i*5:(i+1)*5]\n",
    "        color = torch.tensor(candidate_i[2:]).view(3, 1, 1).to(x.device)\n",
    "        xi, yi = int(candidate_i[0]), int(candidate_i[1])\n",
    "        x_adv[i, :, xi, yi] = color\n",
    "\n",
    "    save_adv_batch(batch=x_adv, pth=pth)\n",
    "    return x_adv, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eba1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    fgsm_kl_attack(model=vit, x=batch, epsilon=4/255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
